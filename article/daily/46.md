### 智能计算系统 实验3

这里主要说一下在做这个实验的过程中想到的一些东西。大体上就是通过贡献来实现代码，总而获得对这些实现的整体的理解，而非数学公式上的理解。

全连接层就是最基础的矩阵乘法，调用numpy给的矩阵乘法就行

在反向传播这里就可以开始考虑贡献了

对于这一层全连接层来说，输入为input，输出为output，所以输入的数据大小就是[batch, input]，而输出的数据大小则是[batch, output]

我们的权重矩阵则是[input, output]

对于我们这一层的损失，也就是上一层对输入的损失，大小就是和对应的输出相同，在这里就是[batch, output]

那这时候我们就可以去想一下，这个下一层的损失，后面我们称为top_diff，是怎么影响到这一层的权重和输入的呢？

我们知道，原始输入的每一行就是一个batch中的一个数据，我们就说是第i行吧，根据矩阵乘法，这第i行的数据，经过和权重矩阵的矩阵乘法，会对输出的第i行造成贡献

而我们在使用这第i行的数据的时候，使用了整个权重矩阵，所以整个权重矩阵也会受到top_diff的第i行的影响

我们可以发现。如果我们的第i行的输入，和第j列的权重矩阵的点积得到了最终的[i, j]元素，那么[i, j]元素自然会对输入的第i行和权重矩阵的第j列有贡献

再靠近点看，对于权重矩阵的每一个元素，他会对谁造成贡献？

比如对于权重矩阵的第[i, j]个元素，他只会与每一个batch中的第i个元素相乘，并对对应batch中的第j个元素造成贡献

这里我们稍微具体一点，对于每一个batch中的元素x[b, i]，他与权重矩阵中的第[i, j]个元素相乘，得到了y[b, j]

所以很显然了，元素w[i, j]与x[b, i]和y[b, j]有关，同时注意我们这里说的是每个，所以对于每个b，我们要进行累计

那我们就可以大概的理解到，w[i, j]的梯度一定与x[b, i]和y[b, j]相关。稍微猜测，或者推导一下我们也可以明白这里是与他们两个的乘积相关

在加上需要累加b这一维的性质，我们可以大概的推导出，dw = x.T * dy

当然这里可能存在系数，但是这里是给大家用相对感性的方式去理解更新梯度的过程

上面的这些步骤其实就是基于了一点基本事实，一个元素x只有对另一个元素y造成贡献时，dy才会对dx造成贡献

然后是正题一点的东西

img2col的卷积加速就是将原始的矩阵展平，